P8105\_hw3\_jl5172
================

Load package and data cleaning

``` r
library(tidyverse)
```

    ## ── Attaching packages ──────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ─────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
data("brfss_smart2010")
brfss<-janitor::clean_names(brfss_smart2010) #clean names
brfss<-rename(brfss,state=locationabbr,location=locationdesc) # use appropriate variable names
brfss<-filter(brfss,topic == "Overall Health") # fiocusing on Overall Health
brfss<-filter(brfss,response=="Excellent"|response=="Very good"|response=="Good"|response=="Fair"|response=="Poor") #Keep responses required
mutate(brfss,response=as.factor(brfss$response)) 
```

    ## # A tibble: 10,625 x 23
    ##     year state location class topic question response sample_size
    ##    <int> <chr> <chr>    <chr> <chr> <chr>    <fct>          <int>
    ##  1  2010 AL    AL - Je… Heal… Over… How is … Excelle…          94
    ##  2  2010 AL    AL - Je… Heal… Over… How is … Very go…         148
    ##  3  2010 AL    AL - Je… Heal… Over… How is … Good             208
    ##  4  2010 AL    AL - Je… Heal… Over… How is … Fair             107
    ##  5  2010 AL    AL - Je… Heal… Over… How is … Poor              45
    ##  6  2010 AL    AL - Mo… Heal… Over… How is … Excelle…          91
    ##  7  2010 AL    AL - Mo… Heal… Over… How is … Very go…         177
    ##  8  2010 AL    AL - Mo… Heal… Over… How is … Good             224
    ##  9  2010 AL    AL - Mo… Heal… Over… How is … Fair             120
    ## 10  2010 AL    AL - Mo… Heal… Over… How is … Poor              66
    ## # ... with 10,615 more rows, and 15 more variables: data_value <dbl>,
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

``` r
brfss$response<-fct_relevel(brfss$response,"Excellent","Very good","Good","Fair","Poor") # organize response as facctors in required ordering
```

In 2002, which states were observed at 7 locations?

``` r
brfss %>% 
  filter(year=="2002") %>%   #filter by year 2002
  distinct(state,location) %>%  # distinct by state and location
  count(state) %>%  # count the number of observations for each states
  filter(n==7)  # looking for states which have been observed for 7 locations
```

    ## # A tibble: 3 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 CT        7
    ## 2 FL        7
    ## 3 NC        7

We can see that CT,FL and NC were observed at 7 location in 2002.

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

``` r
brfss %>% 
  group_by(state,year) %>%  # group by state and year
  distinct(location) %>%   #distinct by locations
  count(state) %>%  #count how many locations in each state were observed in each year
  ggplot(aes(x = year, y = n)) +  #use ggplot to plot spaghetti
    geom_line(aes(color = state))+  # color each line by state 
  labs(
    title = "Number Of Locations in Each States 2002-2010",
    x = "Year",
    y = "Number of Locations",
    caption = "Data from the p8105.datasets package"
  ) +
  viridis::scale_color_viridis(
    name = "", 
    discrete = TRUE)+
  theme(legend.position = "right")
```

![](p8105_hw3_jl5172_files/figure-markdown_github/unnamed-chunk-3-1.png)

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
brfss %>% 
  filter(year=="2002"|year=="2006"|year=="2010",state=="NY",response=="Excellent") %>% 
  group_by(year) %>% 
  summarize(
          mean_proportion_excellent_NY =round(mean(data_value)/100,3),
          sd_excellent_response=round(sd(data_value),3)) %>% 
  View()
```

We can see from the table that the mean propoortion of excellent response is highest in 2002. There is not much difference between 2006 and 2010. For the variance,2002 is also the highest one.

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

``` r
brfss %>% 
  spread(key= response, value = data_value) %>% 
  janitor::clean_names() %>% 
  group_by(state,year) %>% 
  summarize(excellent_mean=mean(excellent,na.rm = T),
            verygood_mean=mean(very_good,na.rm = T),
            good_mean=mean(good,na.rm=T),
            fair_mean=mean(fair,na.rm=T),
            poor_mean=mean(poor,na.rm=T)
            ) %>% 
 gather(key=response_variable,value=mean_value,excellent_mean:poor_mean) %>% 
  ggplot(aes(x=year,y=mean_value,color=state))+
  geom_line()+
  facet_grid(~response_variable)+
  labs(
    title = "Average Proportion In Each Response Category",
    x = "Mean Proportion",
    y = "Year",
    caption = "Data from the p8105.datasets package"
  )
```

![](p8105_hw3_jl5172_files/figure-markdown_github/unnamed-chunk-5-1.png)

Problem 2
=========

Clean data Use suitable variable names.

``` r
cart<-janitor::clean_names(instacart)
```

write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.

Exploration of the dataset.

``` r
cart %>% 
  distinct(product_id) %>% 
  nrow()  # There are 39123 products
```

    ## [1] 39123

``` r
cart %>% 
  distinct(order_id) %>% 
  nrow()    #There are 131209 orders.
```

    ## [1] 131209

``` r
cart %>% 
  distinct(department) %>% 
  nrow()  # There are 21 departmets.
```

    ## [1] 21

According to my basic analysis, there are 39123 distinct products being ordered, 131209 orders have been placed and 21 departments.The column 'order\_dow' has been changed to order\_day(which day of the week), 0 represent Sunday ,1 represents Monday,etc...In the column reordered,the corresponding value is 1 if this product has been ordered by this user in the past,0 otherwise.The column add\_to\_cart\_order means order in which each product was added to cart.Key variables are order\_id, product\_id, reordered, user\_id, order\_number as they are some essential information for us to observe the shopping behaviors of the users in Instacart.

How many aisles are there?

``` r
cart %>% 
  distinct(aisle_id) %>% 
  nrow()
```

    ## [1] 134

We can see that their are 134 distinct id in the column aisle\_id thus there are 134 aiseles there.

which aisles are the most items ordered from?

``` r
aisles_by_norder<-cart %>% 
  group_by(aisle) %>% 
  summarise(most_order_aisle=n()) %>% # use n() to summarize how many  times does each aisle_id appear.
  arrange(desc(most_order_aisle)) %>%  #arrange them in descending order
  as.tibble() 
  
famous_aisles<-aisles_by_norder$aisle[1:5] # famous aisle would appear on top of most_order_aisle.
famous_aisles
```

    ## [1] "fresh vegetables"           "fresh fruits"              
    ## [3] "packaged vegetables fruits" "yogurt"                    
    ## [5] "packaged cheese"

We can see that the most famous aisle is "fresh vegetables" with 150609 items ordered from that aisle.The top 5 famous aisles are"fresh fruits","packaged vegetables fruits","yogurt" and "packaged cheese".

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

``` r
cart %>% 
  group_by(aisle) %>% 
  summarize(number_items_aisle = n()) %>% 
  arrange(desc(number_items_aisle)) %>% 
  ggplot(aes(x = reorder(aisle, -number_items_aisle), y = number_items_aisle, fill = aisle)) +
  geom_bar(stat="identity")+
  coord_flip()+
  theme(text = element_text(size = 6), plot.title = element_text(hjust = 0.5), legend.position = "hide", title = element_text(size = 10,face='bold'))+
  labs(title="Number of Items Ordered in Each Aisle",x="Aisle",y="Number of Items",caption = "Data from p8105.datasets package")
```

![](p8105_hw3_jl5172_files/figure-markdown_github/unnamed-chunk-10-1.png)

Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

``` r
cart %>% 
  filter(aisle=="baking ingredients"|aisle=="dog food care"|aisle=="packaged vegetables fruits") %>%  #filter only these three aisles out
  group_by(aisle,product_name) %>% # pre-group based on aisle and product name
  summarise(n=n()) %>% # summarize number of order per product
  top_n(1,n) %>%  #return 1 row per group, order = 'n' column
  as.tibble() # form a table
```

    ## # A tibble: 3 x 3
    ## # Groups:   aisle [3]
    ##   aisle                    product_name                                  n
    ##   <chr>                    <chr>                                     <int>
    ## 1 baking ingredients       Light Brown Sugar                           499
    ## 2 dog food care            Snack Sticks Chicken & Rice Recipe Dog T…    30
    ## 3 packaged vegetables fru… Organic Baby Spinach                       9784

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

``` r
apple_coffee<-cart %>% 
  filter(product_name=="Pink Lady Apples"|product_name=="Coffee Ice Cream") %>% 
  select(product_name,order_hour_of_day,order_dow) %>% 
  group_by(product_name,order_dow) %>% 
  summarise(mean_time_order=mean(order_hour_of_day)) %>% 
  spread(key=order_dow,value=mean_time_order) %>% 
  as.tibble()

colnames(apple_coffee)<-c("Product Name","Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")
  apple_coffee
```

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   `Product Name`   Sunday Monday Tuesday Wednesday Thursday Friday Saturday
    ##   <chr>             <dbl>  <dbl>   <dbl>     <dbl>    <dbl>  <dbl>    <dbl>
    ## 1 Coffee Ice Cream   13.8   14.3    15.4      15.3     15.2   12.3     13.8
    ## 2 Pink Lady Apples   13.4   11.4    11.7      14.2     11.6   12.8     11.9

Problem 3
=========

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):

``` r
ny<-janitor::clean_names(ny_noaa)
nrow(ny)
```

    ## [1] 2595176

``` r
ncol(ny)
```

    ## [1] 7

This is a dataset which contains 7 variables and 2595176 rows with weather information from 1981-2010. Each row contain information from distinct New York weather station with weather station ID,data of obeservation, precipitation(tenths of mm),snowfall(mm),snow depth(mm),maximum temperature(tenths degrees C),minimum temperature(tenths of degrees C).

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

``` r
ny<-ny %>% 
separate(date,c("year", "month", "day"), sep = "-") %>% 
mutate(prcp=as.numeric(prcp)/10,
tmax=as.numeric(tmax)/10,
tmin=as.numeric(tmin)/10) %>% 
rename(precipitation_mm=prcp) %>% 
rename(tmin_C=tmin) %>% 
rename(tmax_C=tmax) %>% 
rename(snowfall_mm=snow) %>% 
rename(snow_depth_mm=snwd)
```

For snowfall, what are the most commonly observed values? Why?

``` r
ny %>% 
  group_by(snowfall_mm) %>% 
  summarize(n_snowfall=n()) %>% 
  arrange(desc(n_snowfall)) %>% 
  top_n(5)
```

    ## Selecting by n_snowfall

    ## # A tibble: 5 x 2
    ##   snowfall_mm n_snowfall
    ##         <int>      <int>
    ## 1           0    2008508
    ## 2          NA     381221
    ## 3          25      31022
    ## 4          13      23095
    ## 5          51      18274

For snowfall,the most commonly observed value is 0mm because for most of the day, there is no snowfall in New York. 25mm(2nd),13mm(3rd) and 51mm(4th) are also observed frequently.

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

``` r
ny %>% 
  filter(month=="01"|month=="07",!is.na(tmax_C)) %>% 
  mutate(month=recode(month,"01"="Jan")) %>% 
  mutate(month=recode(month,"07"="July")) %>% 
  group_by(id,year,month) %>% 
  summarize(mean_tmax_C=mean(tmax_C)) %>% 
  ggplot(aes(x=year,y=mean_tmax_C),fill=month)+
  geom_boxplot()+
  facet_grid(~month)+
  scale_x_discrete(breaks = c(1981, 1985, 1989, 1993, 1997, 2001,2005,2010))+
  labs(title="Boxplots For Average Max Temperation In Jan & July Across 1981-2010",x="Year",y="Average Maximum Temperature/C",caption = "Data from p8105.datasets package")
```

![](p8105_hw3_jl5172_files/figure-markdown_github/unnamed-chunk-16-1.png) Generally Speaking,the avarage maximum temperatures for July across years are much higher than that of January.There are outliers in almost each year/month.

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option);

``` r
library(ggridges)
```

    ## 
    ## Attaching package: 'ggridges'

    ## The following object is masked from 'package:ggplot2':
    ## 
    ##     scale_discrete_manual

``` r
library(ggplot2)
library(hexbin)
ny %>% 
  filter(!is.na(tmax_C),!is.na(tmin_C)) %>% 
  ggplot(aes(x=tmin_C,y=tmax_C))+
  geom_hex()+labs(title="Hexplot of Maximum Temperature",x="Minimum Temperature/°C",y="Maximum Temperature/°C",caption = "Data from the ny_noaa package")
```

![](p8105_hw3_jl5172_files/figure-markdown_github/unnamed-chunk-17-1.png) We can see from the hexplot that maximum temperature of a day are generally proprotional with minimum temperatures.The most frequent range of minimum temperatures lied between -15°C and 20°C. The most frequent range of maximum temperatures lied between -5°C and 30°C.

Make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

``` r
ny %>% 
  filter(!is.na(snowfall_mm),snowfall_mm>0,snowfall_mm<100) %>% 
  ggplot(aes(x = snowfall_mm,y=year,color=year)) +
  geom_density_ridges(alpha = .5, scale = 0.85) +
  labs(title="Density Plot of Snowfall by Year",x="Snowfall/mm",y="Year",
       caption = "Data from the ny_noaa package")
```

    ## Picking joint bandwidth of 3.76

![](p8105_hw3_jl5172_files/figure-markdown_github/unnamed-chunk-18-1.png) In this density plot, each line represent the snowfall distribution for that year.We can see that the most frequent snowfall(mm) is about 0-30mm for each year.
